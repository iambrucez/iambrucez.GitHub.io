4:I[9275,[],""]
5:I[1343,[],""]
6:I[2224,["291","static/chunks/291-de708f65a06c2e3a.js","348","static/chunks/app/project/layout-92e45420a3e6a1a7.js"],"default"]
7:I[1804,["185","static/chunks/app/layout-561c173eda5e1936.js"],"default"]
2:T76b,def topological_sort(variable: Variable) -> Iterable[Variable]:
    """
    Computes the topological order of the computation graph.

    Args:
        variable: The right-most variable

    Returns:
        Non-constant Variables in topological order starting from the right.
    """
    visited: List[int] = list()
    ordered_vars: List[Variable] = list()

    def visit(variable: Variable) -> None:
        if variable.is_constant() or variable.unique_id in visited:
            return
        if not variable.is_leaf():
            for input_var in variable.parents:
                visit(input_var)
        visited.append(variable.unique_id)
        ordered_vars.insert(0, variable)

    visit(variable)
    return ordered_vars


def backpropagate(variable: Variable, deriv: Any) -> None:
    """
    Runs backpropagation on the computation graph in order to
    compute derivatives for the leave nodes.

    Args:
        variable: The right-most variable
        deriv  : Its derivative that we want to propagate backward to the leaves.

    No return. Should write to its results to the derivative values of 
    each leaf through `accumulate_derivative`.
    """
    ordered_vars: Iterable[Variable] = topological_sort(variable)
    # Record the derivative of each variable
    derivatives: Dict[int, Any] = {var.unique_id: 0 for var in ordered_vars}
    derivatives[variable.unique_id] = deriv

    for var in ordered_vars:
        if var.is_leaf():
            var.accumulate_derivative(derivatives[var.unique_id])
        else:
            for parent_var, deriv in var.chain_rule(derivatives[var.unique_id]):
                if parent_var.is_constant():
                    continue
                if parent_var.unique_id in derivatives:
                    derivatives[parent_var.unique_id] += deriv
                else:
                    derivatives[parent_var.unique_id] = deriv
3:T470,def shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape:
    """
    Broadcast two shapes to create a new union shape.

    Args:
        shape1 : first shape
        shape2 : second shape

    Returns:
        broadcasted shape

    Raises:
        IndexingError : if cannot broadcast
    """
    # Rule 3: extra dimensions of 1 can be implicitly added to the left of the shape.
    if len(shape1) > len(shape2):
        shape2 = [1 for _ in range(len(shape1) - len(shape2))] + list(shape2)
    else:
        shape1 = [1 for _ in range(len(shape2) - len(shape1))] + list(shape1)
    # Now, shape1 and shape2 have the same dimension.
    n_shape: List[int] = []
    for i in range(len(shape1)):
        if shape1[i] != shape2[i]:
            # Rule 1: dimension of size 1 broadcasts with anything
            if shape1[i] == 1:
                n_shape.append(shape2[i])
            elif shape2[i] == 1:
                n_shape.append(shape1[i])
            else:
                raise IndexingError(f"Cannot broadcast {shape1} and {shape2}.")
        else:
            n_shape.append(shape1[i])
    return tuple(n_shape)
0:["2W3RvEAKH_tziCkIg_unr",[[["",{"children":["project",{"children":["minitorch",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",{"children":["project",{"children":["minitorch",{"children":["__PAGE__",{},[["$L1",[["$","blockquote",null,{"children":["\n",["$","p",null,{"children":"\"Hmm, I wonder how PyTorch works?\""}],"\n"]}],"\n",["$","p",null,{"children":["$","a",null,{"href":"https://github.com/iamyufan/minitorch","children":"[GitHub Repo]"}]}],"\n",["$","hr",null,{}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"miniTorch"}]," is a Python re-implementation of the ",["$","a",null,{"href":"http://www.pytorch.org/","children":"Torch"}]," API designed to be simple, easy-to-read, tested, and incremental. The final library can run Torch code. It is designed for me and anyone like me who wants to learn how deep learning libraries work under the hood. It is not intended to be super fast and well-optimized, but the implementation is still considerably efficient."]}],"\n",["$","h2",null,{"children":"Feature Highlights"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","strong",null,{"children":"Build models in an OOP way, just like PyTorch"}]}],"\n",["$","li",null,{"children":["$","strong",null,{"children":"Variable wrapper around NumPy tensor"}]}],"\n",["$","li",null,{"children":["$","strong",null,{"children":"Automatic differentiation"}]}],"\n",["$","li",null,{"children":["$","strong",null,{"children":"JIT compilation"}]}],"\n",["$","li",null,{"children":["$","strong",null,{"children":"GPU acceleration"}]}],"\n"]}],"\n",["$","h2",null,{"children":"Overview"}],"\n",["$","p",null,{"children":["Before diving into the implementation details, let's take a quick recap of the basic concepts in deep learning. In deep learning, we usually have a ",["$","strong",null,{"children":"model"}]," defined as a certain ",["$","strong",null,{"children":"network structure"}]," plus the ",["$","strong",null,{"children":"parameters"}]," associated with the network. The model is trained by feeding some ",["$","strong",null,{"children":"input data"}],", performing some ",["$","strong",null,{"children":"computation"}]," on the input data according to the defined network structure and the parameters, producing some ",["$","strong",null,{"children":"output data"}],", comparing the output data with the ",["$","strong",null,{"children":"ground truth"}],", and updating the model parameters to minimize the ",["$","strong",null,{"children":"loss"}]," between the output data and the ground truth. This process is called ",["$","strong",null,{"children":"backpropagation"}],", which is essentially the chain rule in calculus."]}],"\n",["$","iframe",null,{"style":{"border":0},"width":"100%","height":"100%","src":"https://whimsical.com/embed/Eva4EfzEfVxmCKtPu2xwqF"}],"\n",["$","p",null,{"children":["$","em",null,{"children":"Overview of Training a Deep Learning Model"}]}],"\n",["$","p",null,{"children":"So, given the above overview, we can see that the core of a deep learning library is to provide a way to define a model, perform computation on the model, and compute the gradients of the model parameters. To follow the implementation of PyTorch, miniTorch has the following TODOs:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"We need a data structure to represent the model"}],": miniTorch implements a tree data structure ",["$","code",null,{"children":"Module"}]," that can be ",["$","code",null,{"children":"walked"}]," to find all of the parameters."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"We need to extend the functionality of a numerical value in Python"}],": miniTorch implements a ",["$","code",null,{"children":"Variable"}]," class that wraps a NumPy data structure and provides additional functionalities for automatic differentiation such as storing the function that produces the value."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"We need to implement the computation graph"}],": By connecting the variables through the functions between them, we can build a computation graph. miniTorch implements backpropagation by traversing the computation graph in reverse topological order."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"We need to improve the efficiency"}],": First, miniTorch implements a ",["$","code",null,{"children":"Tensor"}]," class that avoids unnecessary ",["$","code",null,{"children":"for"}]," loops. Second, miniTorch needs parallel computation for it to be fast on CPU as well as GPU."]}],"\n"]}],"\n",["$","h2",null,{"children":"0. Module and Parameter"}],"\n",["$","p",null,{"children":"A deep learning model is often huge in size and complex in structure. For example, there are 1.7 trillion parameters in GPT-4. Also, it is common that a large and complex model contains several repeated structures such as Transformer blocks in GPT-4. A data structure is, therefore, needed for handling deep learning models that can abstract the model structure and parameters."}],"\n",["$","p",null,{"children":["In software development, ",["$","strong",null,{"children":"modular programming"}]," has been a common practice for a long time. It is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules. In deep learning, we can also apply this technique to the model design. We can define a module as a self-contained piece of a model that can be connected to other modules to form a larger model."]}],"\n",["$","p",null,{"children":[["$","code",null,{"children":"Module"}]," is a ",["$","strong",null,{"children":"recursive tree data structure"}],". Inside each ",["$","code",null,{"children":"Module"}],", we have the parameters and other sub-modules. This design is very similar to the tree data structure. We can define a ",["$","code",null,{"children":"Module"}]," class that has a list of sub-modules and a list of parameters. We can also define a ",["$","code",null,{"children":"Parameter"}]," class that has a value and a gradient. The ",["$","code",null,{"children":"Module"}]," class can be ",["$","code",null,{"children":"walked"}]," to find all of the parameters."]}],"\n",["$","p",null,{"children":["Let's take a look at an example of using ",["$","code",null,{"children":"Module"}]," and ",["$","code",null,{"children":"Parameter"}]," to define a model."]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class MyModule(minitorch.Module):\n    def __init__(self):\n        super().__init__()\n        self.parameter1 = minitorch.Parameter(15)\n        self.apple1 = Apple()\n        self.banana2 = Banana()\n\nclass Apple(minitorch.Module):\n    def __init__(self):\n        super().__init__()\n        self.banana1 = Banana()\n        self.banana2 = Banana()\n        self.sweet = minitorch.Parameter(400)\n\nclass Banana(minitorch.Module):\n    def __init__(self):\n        super().__init__()\n        self.yellow = minitorch.Parameter(-92)\n"}]}],"\n",["$","p",null,{"children":["We can print the parameters of module ",["$","code",null,{"children":"MyModule"}]," like we are printing a tree."]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-json","children":"{\n  \"parameter1\": \"15\",\n  \"apple1.sweet\": \"400\",\n  \"apple1.banana1.yellow\": \"-92\",\n  \"apple1.banana2.yellow\": \"-92\",\n  \"banana2.yellow\": \"-92\"\n}\n"}]}],"\n",["$","p",null,{"children":"Below is the tree structure of the module."}],"\n",["$","p",null,{"children":["$","img",null,{"src":"/projects/minitorch/module_tree.png","alt":"Module Tree"}]}],"\n",["$","h2",null,{"children":"1. Automatic Differentiation through a Variable Wrapper"}],"\n",["$","p",null,{"children":["To update the parameters (a.k.a. learn the model), we need to compute the gradients of the parameters given the computed loss and update them for ",["$","strong",null,{"children":"gradient descent"}],". The gradients can be computed by ",["$","strong",null,{"children":"backpropagation"}],", which is essentially the chain rule in calculus. The chain rule is a method for finding the derivative of composite functions. For example, given a composite function ",["$","code",null,{"children":"f(g(x))"}],", the derivative of ",["$","code",null,{"children":"f(g(x))"}]," is ",["$","code",null,{"children":"f'(g(x)) * g'(x)"}],". The chain rule can be applied to the computation graph of a deep learning model. The computation graph is a ",["$","strong",null,{"children":"directed acyclic graph (DAG)"}]," where the nodes are the variables and the edges are the functions that produce the variables."]}],"\n",["$","p",null,{"children":["But there is one gigantic problem remaining: ",["$","strong",null,{"children":"how to compute the derivative of a function?"}]," There are several options:"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"symbolic differentiation"}],": By deriving the derivative of a function symbolically, we can compute the derivative of the function. However, it is not always easy to derive the derivative of a function symbolically."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"numerical differentiation"}],": Since the derivative of a function is the slope of the tangent line of the function, we can approximate the derivative of a function by computing the difference between two points on the function. However, it might be ",["$","strong",null,{"children":"costly"}]," since we need to compute the function twice. Below is the code snippet of numerical differentiation."]}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -> Any:\n    \"\"\"\n    Args:\n        f : arbitrary function from n-scalar args to one value\n        *vals : n-float values $x_0 \\ldots x_{n-1}$\n        arg : the number $i$ of the arg to compute the derivative\n        epsilon : a small constant\n\n    Returns:\n        An approximation of $f'_i(x_0, \\ldots, x_{n-1})$\n    \"\"\"\n    # f(x + epsilon)\n    x_vals_1: List[Any] = list(vals)\n    x_vals_1[arg] = x_vals_1[arg] + epsilon\n    # f(x - epsilon)\n    x_vals_2: List[Any] = list(vals)\n    x_vals_2[arg] = x_vals_2[arg] - epsilon\n\n    return (f(*x_vals_1) - f(*x_vals_2)) / (2 * epsilon)\n"}]}],"\n",["$","p",null,{"children":["In miniTorch and other DL libraries, we adopt another solution, namely ",["$","strong",null,{"children":"automatic differentiation"}],". The idea is to decompose a function and gather the steps about the computational paths within a function. Then, we can compute the derivative of each elementary function and apply the chain rule to compute the derivative of the composite function."]}],"\n",["$","p",null,{"children":["For example, if we are going to calculate the derivative of the function, ",["$","code",null,{"children":"(x * x) * y + 10.0 * x + y"}],", w.r.t. both ",["$","code",null,{"children":"x"}]," and ",["$","code",null,{"children":"y"}],", we can decompose the function into several elementary functions and construct the computation path as follows (ignore the random numbers inside the circles):"]}],"\n",["$","p",null,{"children":["$","img",null,{"src":"/projects/minitorch/computation_graph.png","alt":"Computation Graph"}]}],"\n",["$","p",null,{"children":"However, in order to build such a computation path, we need to trace all the internal computation of any function - which can be difficult to do because Python does not directly expose how its inputs are used in the function, i.e. given the input of a function, what we get is only the output, but not the calculation process of the input inside the function."}],"\n",["$","p",null,{"children":"The solution is quite beautiful:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["Build a new data structure ",["$","code",null,{"children":"Variable"}]," to replace the original numerical value in Python."]}],"\n",["$","li",null,{"children":["Overload the operators in Python with ",["$","code",null,{"children":"Functions"}]," to make the ",["$","code",null,{"children":"Variable"}]," class behave like a numerical value."]}],"\n",["$","li",null,{"children":["Extend the functionality of the ",["$","code",null,{"children":"Variable"}]," class to store the function that produces the value."]}],"\n"]}],"\n",["$","p",null,{"children":["One of the benefits of such a design is that for the user, the ",["$","code",null,{"children":"Variable"}]," class behaves like a numerical value, so the user can use it just like a numerical value and never need to know what is happening under the hood."]}],"\n",["$","h3",null,{"children":"Implementation of autodiff"}],"\n",["$","p",null,{"children":["To achieve what I just described above, we need to implement the code architecture shown in the flowchart below. The code can be found in these three files: ",["$","a",null,{"href":"https://github.com/iamyufan/minitorch/blob/module-1/minitorch/scalar.py","children":["$","code",null,{"children":"scalar.py"}]}],", ",["$","a",null,{"href":"https://github.com/iamyufan/minitorch/blob/module-1/minitorch/scalar_functions.py","children":["$","code",null,{"children":"scalar_function.py"}]}],", and ",["$","a",null,{"href":"https://github.com/iamyufan/minitorch/blob/module-1/minitorch/autodiff.py","children":["$","code",null,{"children":"autodiff.py"}]}],"."]}],"\n",["$","iframe",null,{"style":{"border":0},"width":"100%","height":"100%","src":"https://whimsical.com/embed/8RmHa5dkmEeZVRfoNxTa2n"}],"\n",["$","p",null,{"children":["$","em",null,{"children":"Implementation of Automatic Differentiation on Scalar"}]}],"\n",["$","p",null,{"children":["The ",["$","strong",null,{"children":["$","code",null,{"children":"ScalarFunction"}]}]," class can be viewed as the ",["$","code",null,{"children":"Functions"}]," class as we described above."]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["It overloads the operators in Python and provides additional functionalities for automatic differentiation such as creating new ",["$","code",null,{"children":"Scalar"}]," objects and storing the function that produces the value."]}],"\n",["$","li",null,{"children":["Inside each inherited class of ",["$","code",null,{"children":"ScalarFunction"}],", (e.g., ",["$","code",null,{"children":"Add"}],", ",["$","code",null,{"children":"Mul"}],", etc.), we need to implement the ",["$","code",null,{"children":"forward"}]," and ",["$","code",null,{"children":"backward"}]," methods. The ",["$","code",null,{"children":"forward"}]," method computes the value of the function given the input ",["$","code",null,{"children":"Scalar"}]," objects. The ",["$","code",null,{"children":"backward"}]," method computes the derivative of the function w.r.t. the input ",["$","code",null,{"children":"Scalar"}]," objects."]}],"\n",["$","li",null,{"children":["After computing the result(s) of the function, ",["$","code",null,{"children":"apply"}]," method also creates a new ",["$","code",null,{"children":"Scalar"}]," object and stores the function and the input scalars that produces the value in the ",["$","code",null,{"children":"Scalar"}]," object. This is the key to automatic differentiation. By storing the function that produces the value, we can trace the computation path and compute the derivative of the composite function by applying the chain rule."]}],"\n",["$","li",null,{"children":["When computing the derivatives, sometimes we need to access the value from the forward pass. We can use the ",["$","code",null,{"children":"ctx"}]," object to store the value from the forward pass and access it in the backward pass."]}],"\n",["$","li",null,{"children":["For example, The derivative of ",["$","code",null,{"children":"sigmoid(x)"}]," with respect to ",["$","code",null,{"children":"x"}]," is ",["$","code",null,{"children":"sigmoid(x) * (1 - sigmoid(x))"}],". Therefore, the ",["$","code",null,{"children":"Sigmoid"}]," class can be implemented as follows:"]}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class Sigmoid(ScalarFunction):\n@staticmethod\ndef forward(ctx: Context, a: float) -> float:\n    sig = operators.sigmoid(a)\n    ctx.save_for_backward(sig)\n    return sig\n\n@staticmethod\ndef backward(ctx: Context, d_output: float) -> float:\n    (sig,) = ctx.saved_values\n    result: float = sig * (1 - sig) * d_output\n    return result\n"}]}],"\n",["$","p",null,{"children":["The core ",["$","strong",null,{"children":["$","code",null,{"children":"Scalar"}]}]," class can be viewed as the ",["$","code",null,{"children":"Variable"}]," class as we described above. It wraps a Python ",["$","code",null,{"children":"float"}]," object and provides additional functionalities for automatic differentiation with overloaded operators."]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["A ",["$","code",null,{"children":"Scalar"}]," object contains a ",["$","code",null,{"children":"data"}]," attribute that stores the value of the ",["$","code",null,{"children":"Scalar"}]," object."]}],"\n",["$","li",null,{"children":["It also stores the ",["$","code",null,{"children":"history"}]," that contains the ",["$","code",null,{"children":"ScalarFunction"}]," that produces the current ",["$","code",null,{"children":"Scalar"}]," object."]}],"\n",["$","li",null,{"children":["By calling the ",["$","code",null,{"children":"chain_rule"}]," method, we can compute the derivative w.r.t. the input ",["$","code",null,{"children":"Scalar"}]," object that produces this ",["$","code",null,{"children":"Scalar"}]," object."]}],"\n"]}],"\n",["$","h3",null,{"children":"Backpropagation"}],"\n",["$","p",null,{"children":["Now, we have a ",["$","strong",null,{"children":"computation graph"}]," constructed by ",["$","code",null,{"children":"Scalar"}]," objects and ",["$","code",null,{"children":"ScalarFunction"}]," objects, and we can compute the ",["$","strong",null,{"children":"derivative at each node"}]," in the computation graph by applying the chain rule. Next, we need to figure out how to ",["$","strong",null,{"children":"backpropagate"}]," the derivative from the output node to the input node. That is, we need to find a way to ",["$","strong",null,{"children":"traverse the computation graph"}]," in reverse order to accumulate the gradients so that we can update the parameters of the DL model (which would be the ",["$","strong",null,{"children":"leaves"}]," of the computation graph)."]}],"\n",["$","p",null,{"children":["We could just apply these rules randomly and process each nodes as they come aggregating the resulted values. However this can be quite inefficient. It is better to wait to call ",["$","code",null,{"children":"backward"}]," until we have accumulated all the values we will need."]}],"\n",["$","p",null,{"children":["To handle this issue, we will process the nodes in ",["$","strong",null,{"children":"topological order"}],". The topological ordering of a directed acyclic graph is an ordering that ensures no node is processed after its ancestor. Once we have the order defined, we process each node one at a time in order."]}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Algorithm"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"$2"}]}],"\n",["$","h2",null,{"children":"2. A More Efficient Variable Wrapper: Tensor"}],"\n",["$","p",null,{"children":["We now have a fully developed autodifferentiation system built around scalars. This system is correct, but it is inefficient during training, since ",["$","strong",null,{"children":"every scalar number requires building an object"}],", and ",["$","strong",null,{"children":"each operation requires storing a graph of all the values"}]," that we have previously created. Moreover, training requires repeating the above operations, and running models, such as a linear model, requires a ",["$","strong",null,{"children":["$","code",null,{"children":"for"}]}]," loop over each of the terms in the network."]}],"\n",["$","p",null,{"children":["In miniTorch, a new variable wrapper class ",["$","code",null,{"children":"Tensor"}]," is implemented to solve these problems. Tensors group together many repeated operations to save Python overhead and to pass off grouped operations to faster implementations."]}],"\n",["$","h3",null,{"children":"What's under the hood of Tensor?"}],"\n",["$","p",null,{"children":["Under the hood, a ",["$","code",null,{"children":"Tensor"}]," is a ",["$","strong",null,{"children":"multi-dimensional array"}]," of numbers. It is a wrapper around a NumPy array. Most importantly, it separates the actual data storage from the tensor object. This allows us to ",["$","strong",null,{"children":"share the data storage"}]," between multiple tensors. For example, we can create a tensor ",["$","code",null,{"children":"a"}]," and a tensor ",["$","code",null,{"children":"b"}]," that is a view of ",["$","code",null,{"children":"a"}],". They share the same data storage. Then, we can perform some operations on ",["$","code",null,{"children":"a"}]," and ",["$","code",null,{"children":"b"}]," without creating new tensors. This is very useful when we are dealing with large tensors."]}],"\n",["$","p",null,{"children":["As shown in the chart below, the ",["$","code",null,{"children":"Tensor"}]," class extends what we have built for ",["$","code",null,{"children":"Scalar"}],"."]}],"\n",["$","iframe",null,{"style":{"border":0},"width":"100%","height":"1250","src":"https://whimsical.com/embed/SH4mczPXAqGbW7CtYJAjM2"}],"\n",["$","p",null,{"children":["$","em",null,{"children":"Implementation of Tensor"}]}],"\n",["$","p",null,{"children":"Notice what is different (marked in purple):"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["We have a ",["$","code",null,{"children":"TensorData"}]," class whose ",["$","code",null,{"children":"_storage"}]," stores the actual data of the tensor. The different tensors can share the same data storage, but come with different ",["$","code",null,{"children":"_stride"}]," and ",["$","code",null,{"children":"_shape"}],"."]}],"\n",["$","li",null,{"children":["We have a ",["$","code",null,{"children":"TensorBackend"}]," class which collects all the functions that can be applied to the tensor. The ",["$","code",null,{"children":"TensorBackend"}]," class is a ",["$","strong",null,{"children":"strategy pattern"}],". It allows us to ",["$","strong",null,{"children":"switch between different implementations"}]," of the tensor functions by changing the ",["$","code",null,{"children":"ops"}]," objects. For example, we can implement the higher-ordered functions in a ",["$","code",null,{"children":"for"}]," loop manner (see ",["$","a",null,{"href":"https://github.com/iamyufan/minitorch/blob/module-3/minitorch/tensor_ops.py","children":["$","code",null,{"children":"tensor_ops.py"}]}],"), or with numba JIT parallel computation (see ",["$","a",null,{"href":"https://github.com/iamyufan/minitorch/blob/module-3/minitorch/fast_ops.py","children":["$","code",null,{"children":"fast_ops.py"}]}],"), or with CUDA GPU computation (see ",["$","a",null,{"href":"https://github.com/iamyufan/minitorch/blob/module-3/minitorch/cuda_ops.py","children":["$","code",null,{"children":"cuda_ops.py"}]}],")."]}],"\n"]}],"\n",["$","p",null,{"children":["Let's take a look at an example of calling ",["$","code",null,{"children":"Permute"}]," on a tensor and how it works in autodiff with the new ",["$","code",null,{"children":"Tensor"}]," class. Inside the ",["$","code",null,{"children":"Permute"}]," Function, the ",["$","code",null,{"children":"forward"}]," will create a new Tensor with the same data storage but different ",["$","code",null,{"children":"_stride"}]," and ",["$","code",null,{"children":"_shape"}],". (i.e. different ",["$","code",null,{"children":"TensorData"}]," but with the same ",["$","code",null,{"children":"_storage"}],"). The ",["$","code",null,{"children":"backward"}]," will do something similar."]}],"\n",["$","p",null,{"children":[["$","code",null,{"children":"tensor_functions.py"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"...\n\nclass Permute(Function):\n    @staticmethod\n    def forward(ctx: Context, a: Tensor, order: Tensor) -> Tensor:\n        ori_order: List[int] = [int(i) for i in order._tensor._storage]\n        ctx.save_for_backward(a, ori_order)\n        return minitorch.Tensor(a._tensor.permute(*ori_order), backend=a.backend)\n\n    @staticmethod\n    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, float]:\n        (a, ori_order) = ctx.saved_tensors\n        order: List[int] = [ori_order.index(i) for i in range(len(ori_order))]\n        grad_output = minitorch.Tensor(\n            grad_output._tensor.permute(*order), backend=a.backend\n        )\n        return grad_output, 0\n...\n"}]}],"\n",["$","p",null,{"children":[["$","code",null,{"children":"tensor.py"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"from .tensor_functions import Permute\n...\nclass Tensor:\n    ...\n    def permute(self, *order: int) -> Tensor:\n        \"Permute tensor dimensions to *order\"\n        return Permute.apply(self, tensor(list(order)))\n    ...\n"}]}],"\n",["$","p",null,{"children":[["$","code",null,{"children":"tensor_data.py"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class TensorData:\n    ...\n    def permute(self, *order: int) -> TensorData:\n        \"\"\"\n        Permute the dimensions of the tensor.\n\n        Args:\n            order (list): a permutation of the dimensions\n\n        Returns:\n            New `TensorData` with the same storage and a new dimension order.\n        \"\"\"\n        assert list(sorted(order)) == list(\n            range(len(self.shape))\n        ), f\"Must give a position to each dimension. Shape: {self.shape} Order: {order}\"\n\n        # TODO: Implement for Task 2.1.\n        return TensorData(\n            storage=self._storage,\n            shape=tuple(self.shape[i] for i in order),\n            strides=tuple(self._strides[i] for i in order),\n        )\n...\n"}]}],"\n",["$","p",null,{"children":["What is a little bit tricky is that unlike Scalar, we might want to deal with operations that work on tensors with different shapes. For example, we might want to add a float value ",["$","code",null,{"children":"1.0"}]," to a tensor of shape ",["$","code",null,{"children":"(3, 2)"}],". In this case, we need to ",["$","strong",null,{"children":"broadcast"}]," the tensors to the same shape. We have the following rules for broadcasting:"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Rule 1"}],": Dimension of size 1 can be broadcasted to any shape."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Rule 2"}],": Extra dimensions of 1 can be added with ",["$","code",null,{"children":"view"}],"."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Rule 2"}],": Zip automatically adds dims of size 1 on the left."]}],"\n"]}],"\n",["$","p",null,{"children":["Therefore, we can implement the ",["$","code",null,{"children":"shape_broadcast"}]," method as follows:"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"$3"}]}],"\n",["$","p",null,{"children":["For example, in the case of linear model, we have the data ",["$","code",null,{"children":"X"}]," of shape ",["$","code",null,{"children":"(B * F)"}],", the weight ",["$","code",null,{"children":"W"}]," of shape ",["$","code",null,{"children":"(F * H)"}],", and the bias ",["$","code",null,{"children":"b"}]," of shape ",["$","code",null,{"children":"(H)"}],". To compute the output ",["$","code",null,{"children":"Y"}]," following ",["$","code",null,{"children":"Y = X @ W + b"}],", we need to follow the following steps (suppose we don't have matrix multiplication implemented):"]}],"\n",["$","p",null,{"children":["$","img",null,{"src":"/projects/minitorch/broadcast.png","alt":"Broadcasting"}]}],"\n",["$","h2",null,{"children":"3. Parallel Computation on CPU and GPU"}],"\n",["$","p",null,{"children":"[To Be Updated]"}]]],null],null]},["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","project","children","minitorch","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","$L6",null,{"children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","project","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],"params":{}}],null],null]},[["$","html",null,{"lang":"en","className":"__className_587f35","children":["$","$L7",null,{"children":[["$","head",null,{}],["$","body",null,{"children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/2a1054080b77ae12.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/b16e5c8f629bec3c.css","precedence":"next","crossOrigin":"$undefined"}]]}]}]]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/493015e455655cf7.css","precedence":"next","crossOrigin":"$undefined"}]],"$L8"]]]]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Yufan Zhang"}],["$","meta","3",{"name":"description","content":"Yufan Zhang's personal website"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
