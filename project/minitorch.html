<!DOCTYPE html><html lang="en" class="__className_587f35"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/2d141e1a38819612-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/projects/thumbnail/minitorch.png"/><link rel="preload" as="image" href="/projects/minitorch/module_tree.png"/><link rel="preload" as="image" href="/projects/minitorch/computation_graph.png"/><link rel="preload" as="image" href="/projects/minitorch/broadcast.png"/><link rel="stylesheet" href="/_next/static/css/493015e455655cf7.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/2a1054080b77ae12.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/b16e5c8f629bec3c.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-83e329079fe6102d.js"/><script src="/_next/static/chunks/fd9d1056-6184565b3c21c232.js" async=""></script><script src="/_next/static/chunks/23-1950a167b2ff5a82.js" async=""></script><script src="/_next/static/chunks/main-app-57dd34352df4a0d5.js" async=""></script><script src="/_next/static/chunks/291-de708f65a06c2e3a.js" async=""></script><script src="/_next/static/chunks/app/project/layout-92e45420a3e6a1a7.js" async=""></script><script src="/_next/static/chunks/app/layout-561c173eda5e1936.js" async=""></script><title>Yufan Zhang</title><meta name="description" content="Yufan Zhang&#x27;s personal website"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body><main class="mainPage_main__MhBEv"><div class="navbar_navbar__GSgRc"><div class="navbar_navbarLeft__KMVRO"><p class="navbar_logo__YN5V1"><a href="/">Yufan(Bruce)</a></p></div><div class="navbar_navbarRight__h780S"><div class="navbar_navLinks__iKcrL"><a href="/"><p class="navbar_homeInactive__hkNpL">Home</p></a><a href="/projects"><p class="navbar_projectsActive__2W2Sa">Projects</p></a><a href="/photography"><p class="navbar_photographyInactive__f_Pc7">Photography</p></a><a href="/about"><p class="navbar_resumeInactive__7maxx">About</p></a></div></div></div><div class="mainPage_titleContainer__wAUr9"><h1>miniTorch</h1></div><div class="mainPage_contentContainer__vSAP1"><div class="detail_mainContainer__Tql_C"><div class="detail_infoContainer__K_uh5"><p>2023</p><p>deep learning / software engineering</p></div><div class="detail_thumbnailContainer__WozO_"><img src="/projects/thumbnail/minitorch.png" alt="minitorch"/></div><div class="detail_contentContainer__h8PK1"><blockquote>
<p>&quot;Hmm, I wonder how PyTorch works?&quot;</p>
</blockquote>
<p><a href="https://github.com/iamyufan/minitorch">[GitHub Repo]</a></p>
<hr/>
<p><strong>miniTorch</strong> is a Python re-implementation of the <a href="http://www.pytorch.org/">Torch</a> API designed to be simple, easy-to-read, tested, and incremental. The final library can run Torch code. It is designed for me and anyone like me who wants to learn how deep learning libraries work under the hood. It is not intended to be super fast and well-optimized, but the implementation is still considerably efficient.</p>
<h2>Feature Highlights</h2>
<ul>
<li><strong>Build models in an OOP way, just like PyTorch</strong></li>
<li><strong>Variable wrapper around NumPy tensor</strong></li>
<li><strong>Automatic differentiation</strong></li>
<li><strong>JIT compilation</strong></li>
<li><strong>GPU acceleration</strong></li>
</ul>
<h2>Overview</h2>
<p>Before diving into the implementation details, let&#x27;s take a quick recap of the basic concepts in deep learning. In deep learning, we usually have a <strong>model</strong> defined as a certain <strong>network structure</strong> plus the <strong>parameters</strong> associated with the network. The model is trained by feeding some <strong>input data</strong>, performing some <strong>computation</strong> on the input data according to the defined network structure and the parameters, producing some <strong>output data</strong>, comparing the output data with the <strong>ground truth</strong>, and updating the model parameters to minimize the <strong>loss</strong> between the output data and the ground truth. This process is called <strong>backpropagation</strong>, which is essentially the chain rule in calculus.</p>
<iframe style="border:0" width="100%" height="100%" src="https://whimsical.com/embed/Eva4EfzEfVxmCKtPu2xwqF"></iframe>
<p><em>Overview of Training a Deep Learning Model</em></p>
<p>So, given the above overview, we can see that the core of a deep learning library is to provide a way to define a model, perform computation on the model, and compute the gradients of the model parameters. To follow the implementation of PyTorch, miniTorch has the following TODOs:</p>
<ul>
<li><strong>We need a data structure to represent the model</strong>: miniTorch implements a tree data structure <code>Module</code> that can be <code>walked</code> to find all of the parameters.</li>
<li><strong>We need to extend the functionality of a numerical value in Python</strong>: miniTorch implements a <code>Variable</code> class that wraps a NumPy data structure and provides additional functionalities for automatic differentiation such as storing the function that produces the value.</li>
<li><strong>We need to implement the computation graph</strong>: By connecting the variables through the functions between them, we can build a computation graph. miniTorch implements backpropagation by traversing the computation graph in reverse topological order.</li>
<li><strong>We need to improve the efficiency</strong>: First, miniTorch implements a <code>Tensor</code> class that avoids unnecessary <code>for</code> loops. Second, miniTorch needs parallel computation for it to be fast on CPU as well as GPU.</li>
</ul>
<h2>0. Module and Parameter</h2>
<p>A deep learning model is often huge in size and complex in structure. For example, there are 1.7 trillion parameters in GPT-4. Also, it is common that a large and complex model contains several repeated structures such as Transformer blocks in GPT-4. A data structure is, therefore, needed for handling deep learning models that can abstract the model structure and parameters.</p>
<p>In software development, <strong>modular programming</strong> has been a common practice for a long time. It is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules. In deep learning, we can also apply this technique to the model design. We can define a module as a self-contained piece of a model that can be connected to other modules to form a larger model.</p>
<p><code>Module</code> is a <strong>recursive tree data structure</strong>. Inside each <code>Module</code>, we have the parameters and other sub-modules. This design is very similar to the tree data structure. We can define a <code>Module</code> class that has a list of sub-modules and a list of parameters. We can also define a <code>Parameter</code> class that has a value and a gradient. The <code>Module</code> class can be <code>walked</code> to find all of the parameters.</p>
<p>Let&#x27;s take a look at an example of using <code>Module</code> and <code>Parameter</code> to define a model.</p>
<pre><code class="language-python">class MyModule(minitorch.Module):
    def __init__(self):
        super().__init__()
        self.parameter1 = minitorch.Parameter(15)
        self.apple1 = Apple()
        self.banana2 = Banana()

class Apple(minitorch.Module):
    def __init__(self):
        super().__init__()
        self.banana1 = Banana()
        self.banana2 = Banana()
        self.sweet = minitorch.Parameter(400)

class Banana(minitorch.Module):
    def __init__(self):
        super().__init__()
        self.yellow = minitorch.Parameter(-92)
</code></pre>
<p>We can print the parameters of module <code>MyModule</code> like we are printing a tree.</p>
<pre><code class="language-json">{
  &quot;parameter1&quot;: &quot;15&quot;,
  &quot;apple1.sweet&quot;: &quot;400&quot;,
  &quot;apple1.banana1.yellow&quot;: &quot;-92&quot;,
  &quot;apple1.banana2.yellow&quot;: &quot;-92&quot;,
  &quot;banana2.yellow&quot;: &quot;-92&quot;
}
</code></pre>
<p>Below is the tree structure of the module.</p>
<p><img src="/projects/minitorch/module_tree.png" alt="Module Tree"/></p>
<h2>1. Automatic Differentiation through a Variable Wrapper</h2>
<p>To update the parameters (a.k.a. learn the model), we need to compute the gradients of the parameters given the computed loss and update them for <strong>gradient descent</strong>. The gradients can be computed by <strong>backpropagation</strong>, which is essentially the chain rule in calculus. The chain rule is a method for finding the derivative of composite functions. For example, given a composite function <code>f(g(x))</code>, the derivative of <code>f(g(x))</code> is <code>f&#x27;(g(x)) * g&#x27;(x)</code>. The chain rule can be applied to the computation graph of a deep learning model. The computation graph is a <strong>directed acyclic graph (DAG)</strong> where the nodes are the variables and the edges are the functions that produce the variables.</p>
<p>But there is one gigantic problem remaining: <strong>how to compute the derivative of a function?</strong> There are several options:</p>
<ul>
<li><strong>symbolic differentiation</strong>: By deriving the derivative of a function symbolically, we can compute the derivative of the function. However, it is not always easy to derive the derivative of a function symbolically.</li>
<li><strong>numerical differentiation</strong>: Since the derivative of a function is the slope of the tangent line of the function, we can approximate the derivative of a function by computing the difference between two points on the function. However, it might be <strong>costly</strong> since we need to compute the function twice. Below is the code snippet of numerical differentiation.</li>
</ul>
<pre><code class="language-python">def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -&gt; Any:
    &quot;&quot;&quot;
    Args:
        f : arbitrary function from n-scalar args to one value
        *vals : n-float values $x_0 \ldots x_{n-1}$
        arg : the number $i$ of the arg to compute the derivative
        epsilon : a small constant

    Returns:
        An approximation of $f&#x27;_i(x_0, \ldots, x_{n-1})$
    &quot;&quot;&quot;
    # f(x + epsilon)
    x_vals_1: List[Any] = list(vals)
    x_vals_1[arg] = x_vals_1[arg] + epsilon
    # f(x - epsilon)
    x_vals_2: List[Any] = list(vals)
    x_vals_2[arg] = x_vals_2[arg] - epsilon

    return (f(*x_vals_1) - f(*x_vals_2)) / (2 * epsilon)
</code></pre>
<p>In miniTorch and other DL libraries, we adopt another solution, namely <strong>automatic differentiation</strong>. The idea is to decompose a function and gather the steps about the computational paths within a function. Then, we can compute the derivative of each elementary function and apply the chain rule to compute the derivative of the composite function.</p>
<p>For example, if we are going to calculate the derivative of the function, <code>(x * x) * y + 10.0 * x + y</code>, w.r.t. both <code>x</code> and <code>y</code>, we can decompose the function into several elementary functions and construct the computation path as follows (ignore the random numbers inside the circles):</p>
<p><img src="/projects/minitorch/computation_graph.png" alt="Computation Graph"/></p>
<p>However, in order to build such a computation path, we need to trace all the internal computation of any function - which can be difficult to do because Python does not directly expose how its inputs are used in the function, i.e. given the input of a function, what we get is only the output, but not the calculation process of the input inside the function.</p>
<p>The solution is quite beautiful:</p>
<ul>
<li>Build a new data structure <code>Variable</code> to replace the original numerical value in Python.</li>
<li>Overload the operators in Python with <code>Functions</code> to make the <code>Variable</code> class behave like a numerical value.</li>
<li>Extend the functionality of the <code>Variable</code> class to store the function that produces the value.</li>
</ul>
<p>One of the benefits of such a design is that for the user, the <code>Variable</code> class behaves like a numerical value, so the user can use it just like a numerical value and never need to know what is happening under the hood.</p>
<h3>Implementation of autodiff</h3>
<p>To achieve what I just described above, we need to implement the code architecture shown in the flowchart below. The code can be found in these three files: <a href="https://github.com/iamyufan/minitorch/blob/module-1/minitorch/scalar.py"><code>scalar.py</code></a>, <a href="https://github.com/iamyufan/minitorch/blob/module-1/minitorch/scalar_functions.py"><code>scalar_function.py</code></a>, and <a href="https://github.com/iamyufan/minitorch/blob/module-1/minitorch/autodiff.py"><code>autodiff.py</code></a>.</p>
<iframe style="border:0" width="100%" height="100%" src="https://whimsical.com/embed/8RmHa5dkmEeZVRfoNxTa2n"></iframe>
<p><em>Implementation of Automatic Differentiation on Scalar</em></p>
<p>The <strong><code>ScalarFunction</code></strong> class can be viewed as the <code>Functions</code> class as we described above.</p>
<ul>
<li>It overloads the operators in Python and provides additional functionalities for automatic differentiation such as creating new <code>Scalar</code> objects and storing the function that produces the value.</li>
<li>Inside each inherited class of <code>ScalarFunction</code>, (e.g., <code>Add</code>, <code>Mul</code>, etc.), we need to implement the <code>forward</code> and <code>backward</code> methods. The <code>forward</code> method computes the value of the function given the input <code>Scalar</code> objects. The <code>backward</code> method computes the derivative of the function w.r.t. the input <code>Scalar</code> objects.</li>
<li>After computing the result(s) of the function, <code>apply</code> method also creates a new <code>Scalar</code> object and stores the function and the input scalars that produces the value in the <code>Scalar</code> object. This is the key to automatic differentiation. By storing the function that produces the value, we can trace the computation path and compute the derivative of the composite function by applying the chain rule.</li>
<li>When computing the derivatives, sometimes we need to access the value from the forward pass. We can use the <code>ctx</code> object to store the value from the forward pass and access it in the backward pass.</li>
<li>For example, The derivative of <code>sigmoid(x)</code> with respect to <code>x</code> is <code>sigmoid(x) * (1 - sigmoid(x))</code>. Therefore, the <code>Sigmoid</code> class can be implemented as follows:</li>
</ul>
<pre><code class="language-python">class Sigmoid(ScalarFunction):
@staticmethod
def forward(ctx: Context, a: float) -&gt; float:
    sig = operators.sigmoid(a)
    ctx.save_for_backward(sig)
    return sig

@staticmethod
def backward(ctx: Context, d_output: float) -&gt; float:
    (sig,) = ctx.saved_values
    result: float = sig * (1 - sig) * d_output
    return result
</code></pre>
<p>The core <strong><code>Scalar</code></strong> class can be viewed as the <code>Variable</code> class as we described above. It wraps a Python <code>float</code> object and provides additional functionalities for automatic differentiation with overloaded operators.</p>
<ul>
<li>A <code>Scalar</code> object contains a <code>data</code> attribute that stores the value of the <code>Scalar</code> object.</li>
<li>It also stores the <code>history</code> that contains the <code>ScalarFunction</code> that produces the current <code>Scalar</code> object.</li>
<li>By calling the <code>chain_rule</code> method, we can compute the derivative w.r.t. the input <code>Scalar</code> object that produces this <code>Scalar</code> object.</li>
</ul>
<h3>Backpropagation</h3>
<p>Now, we have a <strong>computation graph</strong> constructed by <code>Scalar</code> objects and <code>ScalarFunction</code> objects, and we can compute the <strong>derivative at each node</strong> in the computation graph by applying the chain rule. Next, we need to figure out how to <strong>backpropagate</strong> the derivative from the output node to the input node. That is, we need to find a way to <strong>traverse the computation graph</strong> in reverse order to accumulate the gradients so that we can update the parameters of the DL model (which would be the <strong>leaves</strong> of the computation graph).</p>
<p>We could just apply these rules randomly and process each nodes as they come aggregating the resulted values. However this can be quite inefficient. It is better to wait to call <code>backward</code> until we have accumulated all the values we will need.</p>
<p>To handle this issue, we will process the nodes in <strong>topological order</strong>. The topological ordering of a directed acyclic graph is an ordering that ensures no node is processed after its ancestor. Once we have the order defined, we process each node one at a time in order.</p>
<p><strong>Algorithm</strong>:</p>
<pre><code class="language-python">def topological_sort(variable: Variable) -&gt; Iterable[Variable]:
    &quot;&quot;&quot;
    Computes the topological order of the computation graph.

    Args:
        variable: The right-most variable

    Returns:
        Non-constant Variables in topological order starting from the right.
    &quot;&quot;&quot;
    visited: List[int] = list()
    ordered_vars: List[Variable] = list()

    def visit(variable: Variable) -&gt; None:
        if variable.is_constant() or variable.unique_id in visited:
            return
        if not variable.is_leaf():
            for input_var in variable.parents:
                visit(input_var)
        visited.append(variable.unique_id)
        ordered_vars.insert(0, variable)

    visit(variable)
    return ordered_vars


def backpropagate(variable: Variable, deriv: Any) -&gt; None:
    &quot;&quot;&quot;
    Runs backpropagation on the computation graph in order to
    compute derivatives for the leave nodes.

    Args:
        variable: The right-most variable
        deriv  : Its derivative that we want to propagate backward to the leaves.

    No return. Should write to its results to the derivative values of 
    each leaf through `accumulate_derivative`.
    &quot;&quot;&quot;
    ordered_vars: Iterable[Variable] = topological_sort(variable)
    # Record the derivative of each variable
    derivatives: Dict[int, Any] = {var.unique_id: 0 for var in ordered_vars}
    derivatives[variable.unique_id] = deriv

    for var in ordered_vars:
        if var.is_leaf():
            var.accumulate_derivative(derivatives[var.unique_id])
        else:
            for parent_var, deriv in var.chain_rule(derivatives[var.unique_id]):
                if parent_var.is_constant():
                    continue
                if parent_var.unique_id in derivatives:
                    derivatives[parent_var.unique_id] += deriv
                else:
                    derivatives[parent_var.unique_id] = deriv
</code></pre>
<h2>2. A More Efficient Variable Wrapper: Tensor</h2>
<p>We now have a fully developed autodifferentiation system built around scalars. This system is correct, but it is inefficient during training, since <strong>every scalar number requires building an object</strong>, and <strong>each operation requires storing a graph of all the values</strong> that we have previously created. Moreover, training requires repeating the above operations, and running models, such as a linear model, requires a <strong><code>for</code></strong> loop over each of the terms in the network.</p>
<p>In miniTorch, a new variable wrapper class <code>Tensor</code> is implemented to solve these problems. Tensors group together many repeated operations to save Python overhead and to pass off grouped operations to faster implementations.</p>
<h3>What&#x27;s under the hood of Tensor?</h3>
<p>Under the hood, a <code>Tensor</code> is a <strong>multi-dimensional array</strong> of numbers. It is a wrapper around a NumPy array. Most importantly, it separates the actual data storage from the tensor object. This allows us to <strong>share the data storage</strong> between multiple tensors. For example, we can create a tensor <code>a</code> and a tensor <code>b</code> that is a view of <code>a</code>. They share the same data storage. Then, we can perform some operations on <code>a</code> and <code>b</code> without creating new tensors. This is very useful when we are dealing with large tensors.</p>
<p>As shown in the chart below, the <code>Tensor</code> class extends what we have built for <code>Scalar</code>.</p>
<iframe style="border:0" width="100%" height="1250" src="https://whimsical.com/embed/SH4mczPXAqGbW7CtYJAjM2"></iframe>
<p><em>Implementation of Tensor</em></p>
<p>Notice what is different (marked in purple):</p>
<ul>
<li>We have a <code>TensorData</code> class whose <code>_storage</code> stores the actual data of the tensor. The different tensors can share the same data storage, but come with different <code>_stride</code> and <code>_shape</code>.</li>
<li>We have a <code>TensorBackend</code> class which collects all the functions that can be applied to the tensor. The <code>TensorBackend</code> class is a <strong>strategy pattern</strong>. It allows us to <strong>switch between different implementations</strong> of the tensor functions by changing the <code>ops</code> objects. For example, we can implement the higher-ordered functions in a <code>for</code> loop manner (see <a href="https://github.com/iamyufan/minitorch/blob/module-3/minitorch/tensor_ops.py"><code>tensor_ops.py</code></a>), or with numba JIT parallel computation (see <a href="https://github.com/iamyufan/minitorch/blob/module-3/minitorch/fast_ops.py"><code>fast_ops.py</code></a>), or with CUDA GPU computation (see <a href="https://github.com/iamyufan/minitorch/blob/module-3/minitorch/cuda_ops.py"><code>cuda_ops.py</code></a>).</li>
</ul>
<p>Let&#x27;s take a look at an example of calling <code>Permute</code> on a tensor and how it works in autodiff with the new <code>Tensor</code> class. Inside the <code>Permute</code> Function, the <code>forward</code> will create a new Tensor with the same data storage but different <code>_stride</code> and <code>_shape</code>. (i.e. different <code>TensorData</code> but with the same <code>_storage</code>). The <code>backward</code> will do something similar.</p>
<p><code>tensor_functions.py</code>:</p>
<pre><code class="language-python">...

class Permute(Function):
    @staticmethod
    def forward(ctx: Context, a: Tensor, order: Tensor) -&gt; Tensor:
        ori_order: List[int] = [int(i) for i in order._tensor._storage]
        ctx.save_for_backward(a, ori_order)
        return minitorch.Tensor(a._tensor.permute(*ori_order), backend=a.backend)

    @staticmethod
    def backward(ctx: Context, grad_output: Tensor) -&gt; Tuple[Tensor, float]:
        (a, ori_order) = ctx.saved_tensors
        order: List[int] = [ori_order.index(i) for i in range(len(ori_order))]
        grad_output = minitorch.Tensor(
            grad_output._tensor.permute(*order), backend=a.backend
        )
        return grad_output, 0
...
</code></pre>
<p><code>tensor.py</code>:</p>
<pre><code class="language-python">from .tensor_functions import Permute
...
class Tensor:
    ...
    def permute(self, *order: int) -&gt; Tensor:
        &quot;Permute tensor dimensions to *order&quot;
        return Permute.apply(self, tensor(list(order)))
    ...
</code></pre>
<p><code>tensor_data.py</code>:</p>
<pre><code class="language-python">class TensorData:
    ...
    def permute(self, *order: int) -&gt; TensorData:
        &quot;&quot;&quot;
        Permute the dimensions of the tensor.

        Args:
            order (list): a permutation of the dimensions

        Returns:
            New `TensorData` with the same storage and a new dimension order.
        &quot;&quot;&quot;
        assert list(sorted(order)) == list(
            range(len(self.shape))
        ), f&quot;Must give a position to each dimension. Shape: {self.shape} Order: {order}&quot;

        # TODO: Implement for Task 2.1.
        return TensorData(
            storage=self._storage,
            shape=tuple(self.shape[i] for i in order),
            strides=tuple(self._strides[i] for i in order),
        )
...
</code></pre>
<p>What is a little bit tricky is that unlike Scalar, we might want to deal with operations that work on tensors with different shapes. For example, we might want to add a float value <code>1.0</code> to a tensor of shape <code>(3, 2)</code>. In this case, we need to <strong>broadcast</strong> the tensors to the same shape. We have the following rules for broadcasting:</p>
<ul>
<li><strong>Rule 1</strong>: Dimension of size 1 can be broadcasted to any shape.</li>
<li><strong>Rule 2</strong>: Extra dimensions of 1 can be added with <code>view</code>.</li>
<li><strong>Rule 2</strong>: Zip automatically adds dims of size 1 on the left.</li>
</ul>
<p>Therefore, we can implement the <code>shape_broadcast</code> method as follows:</p>
<pre><code class="language-python">def shape_broadcast(shape1: UserShape, shape2: UserShape) -&gt; UserShape:
    &quot;&quot;&quot;
    Broadcast two shapes to create a new union shape.

    Args:
        shape1 : first shape
        shape2 : second shape

    Returns:
        broadcasted shape

    Raises:
        IndexingError : if cannot broadcast
    &quot;&quot;&quot;
    # Rule 3: extra dimensions of 1 can be implicitly added to the left of the shape.
    if len(shape1) &gt; len(shape2):
        shape2 = [1 for _ in range(len(shape1) - len(shape2))] + list(shape2)
    else:
        shape1 = [1 for _ in range(len(shape2) - len(shape1))] + list(shape1)
    # Now, shape1 and shape2 have the same dimension.
    n_shape: List[int] = []
    for i in range(len(shape1)):
        if shape1[i] != shape2[i]:
            # Rule 1: dimension of size 1 broadcasts with anything
            if shape1[i] == 1:
                n_shape.append(shape2[i])
            elif shape2[i] == 1:
                n_shape.append(shape1[i])
            else:
                raise IndexingError(f&quot;Cannot broadcast {shape1} and {shape2}.&quot;)
        else:
            n_shape.append(shape1[i])
    return tuple(n_shape)
</code></pre>
<p>For example, in the case of linear model, we have the data <code>X</code> of shape <code>(B * F)</code>, the weight <code>W</code> of shape <code>(F * H)</code>, and the bias <code>b</code> of shape <code>(H)</code>. To compute the output <code>Y</code> following <code>Y = X @ W + b</code>, we need to follow the following steps (suppose we don&#x27;t have matrix multiplication implemented):</p>
<p><img src="/projects/minitorch/broadcast.png" alt="Broadcasting"/></p>
<h2>3. Parallel Computation on CPU and GPU</h2>
<p>[To Be Updated]</p></div></div></div><div class="footer_footer__zL2zm"><div class="footer_forthContainer__7lvSw"><div class="footer_contactContainer__xj3kr"><p class="footer_link__bJEVc"><a href="mailto:yufanbruce@gmail.com">Email me</a></p><div class="footer_contacts__96ifp"><p class="footer_regText__Qe69z">Or find me on...</p><p class="footer_link__bJEVc"><a href="https://www.linkedin.com/in/yufanbruce/">LinkedIn</a></p><p class="footer_link__bJEVc"><a href="https://github.com/iamyufan">GitHub</a></p><p class="footer_link__bJEVc"><a href="https://www.instagram.com/byeruuuuuuuce/">Instagram</a></p><p class="footer_link__bJEVc"><a href="https://open.spotify.com/user/393p86bewg6tgzebw0xgnh680?si=bfda812f31674a12">Spotify</a></p></div></div></div><p class="footer_copyright__FVm5Y">© 2021 — 2024 Yufan Zhang</p></div></main><script src="/_next/static/chunks/webpack-83e329079fe6102d.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/2d141e1a38819612-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/493015e455655cf7.css\",\"style\"]\n4:HL[\"/_next/static/css/2a1054080b77ae12.css\",\"style\"]\n5:HL[\"/_next/static/css/b16e5c8f629bec3c.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"6:I[5751,[],\"\"]\na:I[9275,[],\"\"]\nb:I[1343,[],\"\"]\nc:I[2224,[\"291\",\"static/chunks/291-de708f65a06c2e3a.js\",\"348\",\"static/chunks/app/project/layout-92e45420a3e6a1a7.js\"],\"default\"]\nd:I[1804,[\"185\",\"static/chunks/app/layout-561c173eda5e1936.js\"],\"default\"]\nf:I[6130,[],\"\"]\n8:T76b,def topological_sort(variable: Variable) -\u003e Iterable[Variable]:\n    \"\"\"\n    Computes the topological order of the computation graph.\n\n    Args:\n        variable: The right-most variable\n\n    Returns:\n        Non-constant Variables in topological order starting from the right.\n    \"\"\"\n    visited: List[int] = list()\n    ordered_vars: List[Variable] = list()\n\n    def visit(variable: Variable) -\u003e None:\n        if variable.is_constant() or variable.unique_id in visited:\n            return\n        if not variable.is_leaf():\n            for input_var in variable.parents:\n                visit(input_var)\n        visited.append(variable.unique_id)\n        ordered_vars.insert(0, variable)\n\n    visit(variable)\n    return ordered_vars\n\n\ndef backpropagate(variable: Variable, deriv: Any) -\u003e None:\n    \"\"\"\n    Runs backpropagation on the computation graph in order to\n    compute derivatives for the leave nodes.\n\n    Args:\n        variable: The right-most variable\n        deriv  : Its derivative that we want to propagate backward to the leaves.\n\n    No return. Should write to its results to the derivative values of \n    each leaf through `accumulate_derivative`.\n    \"\"\"\n    ordered_vars: Iterable[Variable] = topological_sort(variable)\n    # Record the derivative of each variable\n    derivatives: Dict[int, Any] = {var.unique_id: 0 for var in ordered_vars}\n    derivatives[variable.unique_id] = deriv\n\n    for var in ordered_vars:\n        if var.is_leaf():\n            var.accumulate_derivative(derivatives[var.unique_id])\n        else:\n            for parent_var, deriv in var.chain_rule(derivatives[var.unique_id]):\n                if parent_var.is_constant():\n                    continue\n                if parent_var.unique_id in derivatives:\n                    d"])</script><script>self.__next_f.push([1,"erivatives[parent_var.unique_id] += deriv\n                else:\n                    derivatives[parent_var.unique_id] = deriv\n9:T470,def shape_broadcast(shape1: UserShape, shape2: UserShape) -\u003e UserShape:\n    \"\"\"\n    Broadcast two shapes to create a new union shape.\n\n    Args:\n        shape1 : first shape\n        shape2 : second shape\n\n    Returns:\n        broadcasted shape\n\n    Raises:\n        IndexingError : if cannot broadcast\n    \"\"\"\n    # Rule 3: extra dimensions of 1 can be implicitly added to the left of the shape.\n    if len(shape1) \u003e len(shape2):\n        shape2 = [1 for _ in range(len(shape1) - len(shape2))] + list(shape2)\n    else:\n        shape1 = [1 for _ in range(len(shape2) - len(shape1))] + list(shape1)\n    # Now, shape1 and shape2 have the same dimension.\n    n_shape: List[int] = []\n    for i in range(len(shape1)):\n        if shape1[i] != shape2[i]:\n            # Rule 1: dimension of size 1 broadcasts with anything\n            if shape1[i] == 1:\n                n_shape.append(shape2[i])\n            elif shape2[i] == 1:\n                n_shape.append(shape1[i])\n            else:\n                raise IndexingError(f\"Cannot broadcast {shape1} and {shape2}.\")\n        else:\n            n_shape.append(shape1[i])\n    return tuple(n_shape)\n10:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/493015e455655cf7.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"Jel4viYag_xSi-sGrdWnx\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/project/minitorch\",\"initialTree\":[\"\",{\"children\":[\"project\",{\"children\":[\"minitorch\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"project\",{\"children\":[\"minitorch\",{\"children\":[\"__PAGE__\",{},[[\"$L7\",[[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"\\\"Hmm, I wonder how PyTorch works?\\\"\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch\",\"children\":\"[GitHub Repo]\"}]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"miniTorch\"}],\" is a Python re-implementation of the \",[\"$\",\"a\",null,{\"href\":\"http://www.pytorch.org/\",\"children\":\"Torch\"}],\" API designed to be simple, easy-to-read, tested, and incremental. The final library can run Torch code. It is designed for me and anyone like me who wants to learn how deep learning libraries work under the hood. It is not intended to be super fast and well-optimized, but the implementation is still considerably efficient.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Feature Highlights\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Build models in an OOP way, just like PyTorch\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Variable wrapper around NumPy tensor\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Automatic differentiation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"JIT compilation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"GPU acceleration\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Overview\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Before diving into the implementation details, let's take a quick recap of the basic concepts in deep learning. In deep learning, we usually have a \",[\"$\",\"strong\",null,{\"children\":\"model\"}],\" defined as a certain \",[\"$\",\"strong\",null,{\"children\":\"network structure\"}],\" plus the \",[\"$\",\"strong\",null,{\"children\":\"parameters\"}],\" associated with the network. The model is trained by feeding some \",[\"$\",\"strong\",null,{\"children\":\"input data\"}],\", performing some \",[\"$\",\"strong\",null,{\"children\":\"computation\"}],\" on the input data according to the defined network structure and the parameters, producing some \",[\"$\",\"strong\",null,{\"children\":\"output data\"}],\", comparing the output data with the \",[\"$\",\"strong\",null,{\"children\":\"ground truth\"}],\", and updating the model parameters to minimize the \",[\"$\",\"strong\",null,{\"children\":\"loss\"}],\" between the output data and the ground truth. This process is called \",[\"$\",\"strong\",null,{\"children\":\"backpropagation\"}],\", which is essentially the chain rule in calculus.\"]}],\"\\n\",[\"$\",\"iframe\",null,{\"style\":{\"border\":0},\"width\":\"100%\",\"height\":\"100%\",\"src\":\"https://whimsical.com/embed/Eva4EfzEfVxmCKtPu2xwqF\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Overview of Training a Deep Learning Model\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"So, given the above overview, we can see that the core of a deep learning library is to provide a way to define a model, perform computation on the model, and compute the gradients of the model parameters. To follow the implementation of PyTorch, miniTorch has the following TODOs:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"We need a data structure to represent the model\"}],\": miniTorch implements a tree data structure \",[\"$\",\"code\",null,{\"children\":\"Module\"}],\" that can be \",[\"$\",\"code\",null,{\"children\":\"walked\"}],\" to find all of the parameters.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"We need to extend the functionality of a numerical value in Python\"}],\": miniTorch implements a \",[\"$\",\"code\",null,{\"children\":\"Variable\"}],\" class that wraps a NumPy data structure and provides additional functionalities for automatic differentiation such as storing the function that produces the value.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"We need to implement the computation graph\"}],\": By connecting the variables through the functions between them, we can build a computation graph. miniTorch implements backpropagation by traversing the computation graph in reverse topological order.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"We need to improve the efficiency\"}],\": First, miniTorch implements a \",[\"$\",\"code\",null,{\"children\":\"Tensor\"}],\" class that avoids unnecessary \",[\"$\",\"code\",null,{\"children\":\"for\"}],\" loops. Second, miniTorch needs parallel computation for it to be fast on CPU as well as GPU.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"0. Module and Parameter\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A deep learning model is often huge in size and complex in structure. For example, there are 1.7 trillion parameters in GPT-4. Also, it is common that a large and complex model contains several repeated structures such as Transformer blocks in GPT-4. A data structure is, therefore, needed for handling deep learning models that can abstract the model structure and parameters.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In software development, \",[\"$\",\"strong\",null,{\"children\":\"modular programming\"}],\" has been a common practice for a long time. It is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules. In deep learning, we can also apply this technique to the model design. We can define a module as a self-contained piece of a model that can be connected to other modules to form a larger model.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"Module\"}],\" is a \",[\"$\",\"strong\",null,{\"children\":\"recursive tree data structure\"}],\". Inside each \",[\"$\",\"code\",null,{\"children\":\"Module\"}],\", we have the parameters and other sub-modules. This design is very similar to the tree data structure. We can define a \",[\"$\",\"code\",null,{\"children\":\"Module\"}],\" class that has a list of sub-modules and a list of parameters. We can also define a \",[\"$\",\"code\",null,{\"children\":\"Parameter\"}],\" class that has a value and a gradient. The \",[\"$\",\"code\",null,{\"children\":\"Module\"}],\" class can be \",[\"$\",\"code\",null,{\"children\":\"walked\"}],\" to find all of the parameters.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Let's take a look at an example of using \",[\"$\",\"code\",null,{\"children\":\"Module\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"Parameter\"}],\" to define a model.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class MyModule(minitorch.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.parameter1 = minitorch.Parameter(15)\\n        self.apple1 = Apple()\\n        self.banana2 = Banana()\\n\\nclass Apple(minitorch.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.banana1 = Banana()\\n        self.banana2 = Banana()\\n        self.sweet = minitorch.Parameter(400)\\n\\nclass Banana(minitorch.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.yellow = minitorch.Parameter(-92)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We can print the parameters of module \",[\"$\",\"code\",null,{\"children\":\"MyModule\"}],\" like we are printing a tree.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-json\",\"children\":\"{\\n  \\\"parameter1\\\": \\\"15\\\",\\n  \\\"apple1.sweet\\\": \\\"400\\\",\\n  \\\"apple1.banana1.yellow\\\": \\\"-92\\\",\\n  \\\"apple1.banana2.yellow\\\": \\\"-92\\\",\\n  \\\"banana2.yellow\\\": \\\"-92\\\"\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Below is the tree structure of the module.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/projects/minitorch/module_tree.png\",\"alt\":\"Module Tree\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"1. Automatic Differentiation through a Variable Wrapper\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"To update the parameters (a.k.a. learn the model), we need to compute the gradients of the parameters given the computed loss and update them for \",[\"$\",\"strong\",null,{\"children\":\"gradient descent\"}],\". The gradients can be computed by \",[\"$\",\"strong\",null,{\"children\":\"backpropagation\"}],\", which is essentially the chain rule in calculus. The chain rule is a method for finding the derivative of composite functions. For example, given a composite function \",[\"$\",\"code\",null,{\"children\":\"f(g(x))\"}],\", the derivative of \",[\"$\",\"code\",null,{\"children\":\"f(g(x))\"}],\" is \",[\"$\",\"code\",null,{\"children\":\"f'(g(x)) * g'(x)\"}],\". The chain rule can be applied to the computation graph of a deep learning model. The computation graph is a \",[\"$\",\"strong\",null,{\"children\":\"directed acyclic graph (DAG)\"}],\" where the nodes are the variables and the edges are the functions that produce the variables.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"But there is one gigantic problem remaining: \",[\"$\",\"strong\",null,{\"children\":\"how to compute the derivative of a function?\"}],\" There are several options:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"symbolic differentiation\"}],\": By deriving the derivative of a function symbolically, we can compute the derivative of the function. However, it is not always easy to derive the derivative of a function symbolically.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"numerical differentiation\"}],\": Since the derivative of a function is the slope of the tangent line of the function, we can approximate the derivative of a function by computing the difference between two points on the function. However, it might be \",[\"$\",\"strong\",null,{\"children\":\"costly\"}],\" since we need to compute the function twice. Below is the code snippet of numerical differentiation.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -\u003e Any:\\n    \\\"\\\"\\\"\\n    Args:\\n        f : arbitrary function from n-scalar args to one value\\n        *vals : n-float values $x_0 \\\\ldots x_{n-1}$\\n        arg : the number $i$ of the arg to compute the derivative\\n        epsilon : a small constant\\n\\n    Returns:\\n        An approximation of $f'_i(x_0, \\\\ldots, x_{n-1})$\\n    \\\"\\\"\\\"\\n    # f(x + epsilon)\\n    x_vals_1: List[Any] = list(vals)\\n    x_vals_1[arg] = x_vals_1[arg] + epsilon\\n    # f(x - epsilon)\\n    x_vals_2: List[Any] = list(vals)\\n    x_vals_2[arg] = x_vals_2[arg] - epsilon\\n\\n    return (f(*x_vals_1) - f(*x_vals_2)) / (2 * epsilon)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In miniTorch and other DL libraries, we adopt another solution, namely \",[\"$\",\"strong\",null,{\"children\":\"automatic differentiation\"}],\". The idea is to decompose a function and gather the steps about the computational paths within a function. Then, we can compute the derivative of each elementary function and apply the chain rule to compute the derivative of the composite function.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"For example, if we are going to calculate the derivative of the function, \",[\"$\",\"code\",null,{\"children\":\"(x * x) * y + 10.0 * x + y\"}],\", w.r.t. both \",[\"$\",\"code\",null,{\"children\":\"x\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"y\"}],\", we can decompose the function into several elementary functions and construct the computation path as follows (ignore the random numbers inside the circles):\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/projects/minitorch/computation_graph.png\",\"alt\":\"Computation Graph\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"However, in order to build such a computation path, we need to trace all the internal computation of any function - which can be difficult to do because Python does not directly expose how its inputs are used in the function, i.e. given the input of a function, what we get is only the output, but not the calculation process of the input inside the function.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The solution is quite beautiful:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Build a new data structure \",[\"$\",\"code\",null,{\"children\":\"Variable\"}],\" to replace the original numerical value in Python.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Overload the operators in Python with \",[\"$\",\"code\",null,{\"children\":\"Functions\"}],\" to make the \",[\"$\",\"code\",null,{\"children\":\"Variable\"}],\" class behave like a numerical value.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Extend the functionality of the \",[\"$\",\"code\",null,{\"children\":\"Variable\"}],\" class to store the function that produces the value.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"One of the benefits of such a design is that for the user, the \",[\"$\",\"code\",null,{\"children\":\"Variable\"}],\" class behaves like a numerical value, so the user can use it just like a numerical value and never need to know what is happening under the hood.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Implementation of autodiff\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"To achieve what I just described above, we need to implement the code architecture shown in the flowchart below. The code can be found in these three files: \",[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch/blob/module-1/minitorch/scalar.py\",\"children\":[\"$\",\"code\",null,{\"children\":\"scalar.py\"}]}],\", \",[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch/blob/module-1/minitorch/scalar_functions.py\",\"children\":[\"$\",\"code\",null,{\"children\":\"scalar_function.py\"}]}],\", and \",[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch/blob/module-1/minitorch/autodiff.py\",\"children\":[\"$\",\"code\",null,{\"children\":\"autodiff.py\"}]}],\".\"]}],\"\\n\",[\"$\",\"iframe\",null,{\"style\":{\"border\":0},\"width\":\"100%\",\"height\":\"100%\",\"src\":\"https://whimsical.com/embed/8RmHa5dkmEeZVRfoNxTa2n\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Implementation of Automatic Differentiation on Scalar\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"strong\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"ScalarFunction\"}]}],\" class can be viewed as the \",[\"$\",\"code\",null,{\"children\":\"Functions\"}],\" class as we described above.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"It overloads the operators in Python and provides additional functionalities for automatic differentiation such as creating new \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" objects and storing the function that produces the value.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Inside each inherited class of \",[\"$\",\"code\",null,{\"children\":\"ScalarFunction\"}],\", (e.g., \",[\"$\",\"code\",null,{\"children\":\"Add\"}],\", \",[\"$\",\"code\",null,{\"children\":\"Mul\"}],\", etc.), we need to implement the \",[\"$\",\"code\",null,{\"children\":\"forward\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"backward\"}],\" methods. The \",[\"$\",\"code\",null,{\"children\":\"forward\"}],\" method computes the value of the function given the input \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" objects. The \",[\"$\",\"code\",null,{\"children\":\"backward\"}],\" method computes the derivative of the function w.r.t. the input \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" objects.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"After computing the result(s) of the function, \",[\"$\",\"code\",null,{\"children\":\"apply\"}],\" method also creates a new \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object and stores the function and the input scalars that produces the value in the \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object. This is the key to automatic differentiation. By storing the function that produces the value, we can trace the computation path and compute the derivative of the composite function by applying the chain rule.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"When computing the derivatives, sometimes we need to access the value from the forward pass. We can use the \",[\"$\",\"code\",null,{\"children\":\"ctx\"}],\" object to store the value from the forward pass and access it in the backward pass.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"For example, The derivative of \",[\"$\",\"code\",null,{\"children\":\"sigmoid(x)\"}],\" with respect to \",[\"$\",\"code\",null,{\"children\":\"x\"}],\" is \",[\"$\",\"code\",null,{\"children\":\"sigmoid(x) * (1 - sigmoid(x))\"}],\". Therefore, the \",[\"$\",\"code\",null,{\"children\":\"Sigmoid\"}],\" class can be implemented as follows:\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class Sigmoid(ScalarFunction):\\n@staticmethod\\ndef forward(ctx: Context, a: float) -\u003e float:\\n    sig = operators.sigmoid(a)\\n    ctx.save_for_backward(sig)\\n    return sig\\n\\n@staticmethod\\ndef backward(ctx: Context, d_output: float) -\u003e float:\\n    (sig,) = ctx.saved_values\\n    result: float = sig * (1 - sig) * d_output\\n    return result\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The core \",[\"$\",\"strong\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"Scalar\"}]}],\" class can be viewed as the \",[\"$\",\"code\",null,{\"children\":\"Variable\"}],\" class as we described above. It wraps a Python \",[\"$\",\"code\",null,{\"children\":\"float\"}],\" object and provides additional functionalities for automatic differentiation with overloaded operators.\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"A \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object contains a \",[\"$\",\"code\",null,{\"children\":\"data\"}],\" attribute that stores the value of the \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"It also stores the \",[\"$\",\"code\",null,{\"children\":\"history\"}],\" that contains the \",[\"$\",\"code\",null,{\"children\":\"ScalarFunction\"}],\" that produces the current \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"By calling the \",[\"$\",\"code\",null,{\"children\":\"chain_rule\"}],\" method, we can compute the derivative w.r.t. the input \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object that produces this \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" object.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Backpropagation\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Now, we have a \",[\"$\",\"strong\",null,{\"children\":\"computation graph\"}],\" constructed by \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\" objects and \",[\"$\",\"code\",null,{\"children\":\"ScalarFunction\"}],\" objects, and we can compute the \",[\"$\",\"strong\",null,{\"children\":\"derivative at each node\"}],\" in the computation graph by applying the chain rule. Next, we need to figure out how to \",[\"$\",\"strong\",null,{\"children\":\"backpropagate\"}],\" the derivative from the output node to the input node. That is, we need to find a way to \",[\"$\",\"strong\",null,{\"children\":\"traverse the computation graph\"}],\" in reverse order to accumulate the gradients so that we can update the parameters of the DL model (which would be the \",[\"$\",\"strong\",null,{\"children\":\"leaves\"}],\" of the computation graph).\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We could just apply these rules randomly and process each nodes as they come aggregating the resulted values. However this can be quite inefficient. It is better to wait to call \",[\"$\",\"code\",null,{\"children\":\"backward\"}],\" until we have accumulated all the values we will need.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"To handle this issue, we will process the nodes in \",[\"$\",\"strong\",null,{\"children\":\"topological order\"}],\". The topological ordering of a directed acyclic graph is an ordering that ensures no node is processed after its ancestor. Once we have the order defined, we process each node one at a time in order.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Algorithm\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"$8\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"2. A More Efficient Variable Wrapper: Tensor\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We now have a fully developed autodifferentiation system built around scalars. This system is correct, but it is inefficient during training, since \",[\"$\",\"strong\",null,{\"children\":\"every scalar number requires building an object\"}],\", and \",[\"$\",\"strong\",null,{\"children\":\"each operation requires storing a graph of all the values\"}],\" that we have previously created. Moreover, training requires repeating the above operations, and running models, such as a linear model, requires a \",[\"$\",\"strong\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"for\"}]}],\" loop over each of the terms in the network.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In miniTorch, a new variable wrapper class \",[\"$\",\"code\",null,{\"children\":\"Tensor\"}],\" is implemented to solve these problems. Tensors group together many repeated operations to save Python overhead and to pass off grouped operations to faster implementations.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"What's under the hood of Tensor?\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Under the hood, a \",[\"$\",\"code\",null,{\"children\":\"Tensor\"}],\" is a \",[\"$\",\"strong\",null,{\"children\":\"multi-dimensional array\"}],\" of numbers. It is a wrapper around a NumPy array. Most importantly, it separates the actual data storage from the tensor object. This allows us to \",[\"$\",\"strong\",null,{\"children\":\"share the data storage\"}],\" between multiple tensors. For example, we can create a tensor \",[\"$\",\"code\",null,{\"children\":\"a\"}],\" and a tensor \",[\"$\",\"code\",null,{\"children\":\"b\"}],\" that is a view of \",[\"$\",\"code\",null,{\"children\":\"a\"}],\". They share the same data storage. Then, we can perform some operations on \",[\"$\",\"code\",null,{\"children\":\"a\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"b\"}],\" without creating new tensors. This is very useful when we are dealing with large tensors.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"As shown in the chart below, the \",[\"$\",\"code\",null,{\"children\":\"Tensor\"}],\" class extends what we have built for \",[\"$\",\"code\",null,{\"children\":\"Scalar\"}],\".\"]}],\"\\n\",[\"$\",\"iframe\",null,{\"style\":{\"border\":0},\"width\":\"100%\",\"height\":\"1250\",\"src\":\"https://whimsical.com/embed/SH4mczPXAqGbW7CtYJAjM2\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Implementation of Tensor\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Notice what is different (marked in purple):\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"We have a \",[\"$\",\"code\",null,{\"children\":\"TensorData\"}],\" class whose \",[\"$\",\"code\",null,{\"children\":\"_storage\"}],\" stores the actual data of the tensor. The different tensors can share the same data storage, but come with different \",[\"$\",\"code\",null,{\"children\":\"_stride\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"_shape\"}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"We have a \",[\"$\",\"code\",null,{\"children\":\"TensorBackend\"}],\" class which collects all the functions that can be applied to the tensor. The \",[\"$\",\"code\",null,{\"children\":\"TensorBackend\"}],\" class is a \",[\"$\",\"strong\",null,{\"children\":\"strategy pattern\"}],\". It allows us to \",[\"$\",\"strong\",null,{\"children\":\"switch between different implementations\"}],\" of the tensor functions by changing the \",[\"$\",\"code\",null,{\"children\":\"ops\"}],\" objects. For example, we can implement the higher-ordered functions in a \",[\"$\",\"code\",null,{\"children\":\"for\"}],\" loop manner (see \",[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch/blob/module-3/minitorch/tensor_ops.py\",\"children\":[\"$\",\"code\",null,{\"children\":\"tensor_ops.py\"}]}],\"), or with numba JIT parallel computation (see \",[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch/blob/module-3/minitorch/fast_ops.py\",\"children\":[\"$\",\"code\",null,{\"children\":\"fast_ops.py\"}]}],\"), or with CUDA GPU computation (see \",[\"$\",\"a\",null,{\"href\":\"https://github.com/iamyufan/minitorch/blob/module-3/minitorch/cuda_ops.py\",\"children\":[\"$\",\"code\",null,{\"children\":\"cuda_ops.py\"}]}],\").\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Let's take a look at an example of calling \",[\"$\",\"code\",null,{\"children\":\"Permute\"}],\" on a tensor and how it works in autodiff with the new \",[\"$\",\"code\",null,{\"children\":\"Tensor\"}],\" class. Inside the \",[\"$\",\"code\",null,{\"children\":\"Permute\"}],\" Function, the \",[\"$\",\"code\",null,{\"children\":\"forward\"}],\" will create a new Tensor with the same data storage but different \",[\"$\",\"code\",null,{\"children\":\"_stride\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"_shape\"}],\". (i.e. different \",[\"$\",\"code\",null,{\"children\":\"TensorData\"}],\" but with the same \",[\"$\",\"code\",null,{\"children\":\"_storage\"}],\"). The \",[\"$\",\"code\",null,{\"children\":\"backward\"}],\" will do something similar.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"tensor_functions.py\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"...\\n\\nclass Permute(Function):\\n    @staticmethod\\n    def forward(ctx: Context, a: Tensor, order: Tensor) -\u003e Tensor:\\n        ori_order: List[int] = [int(i) for i in order._tensor._storage]\\n        ctx.save_for_backward(a, ori_order)\\n        return minitorch.Tensor(a._tensor.permute(*ori_order), backend=a.backend)\\n\\n    @staticmethod\\n    def backward(ctx: Context, grad_output: Tensor) -\u003e Tuple[Tensor, float]:\\n        (a, ori_order) = ctx.saved_tensors\\n        order: List[int] = [ori_order.index(i) for i in range(len(ori_order))]\\n        grad_output = minitorch.Tensor(\\n            grad_output._tensor.permute(*order), backend=a.backend\\n        )\\n        return grad_output, 0\\n...\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"tensor.py\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"from .tensor_functions import Permute\\n...\\nclass Tensor:\\n    ...\\n    def permute(self, *order: int) -\u003e Tensor:\\n        \\\"Permute tensor dimensions to *order\\\"\\n        return Permute.apply(self, tensor(list(order)))\\n    ...\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"tensor_data.py\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class TensorData:\\n    ...\\n    def permute(self, *order: int) -\u003e TensorData:\\n        \\\"\\\"\\\"\\n        Permute the dimensions of the tensor.\\n\\n        Args:\\n            order (list): a permutation of the dimensions\\n\\n        Returns:\\n            New `TensorData` with the same storage and a new dimension order.\\n        \\\"\\\"\\\"\\n        assert list(sorted(order)) == list(\\n            range(len(self.shape))\\n        ), f\\\"Must give a position to each dimension. Shape: {self.shape} Order: {order}\\\"\\n\\n        # TODO: Implement for Task 2.1.\\n        return TensorData(\\n            storage=self._storage,\\n            shape=tuple(self.shape[i] for i in order),\\n            strides=tuple(self._strides[i] for i in order),\\n        )\\n...\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"What is a little bit tricky is that unlike Scalar, we might want to deal with operations that work on tensors with different shapes. For example, we might want to add a float value \",[\"$\",\"code\",null,{\"children\":\"1.0\"}],\" to a tensor of shape \",[\"$\",\"code\",null,{\"children\":\"(3, 2)\"}],\". In this case, we need to \",[\"$\",\"strong\",null,{\"children\":\"broadcast\"}],\" the tensors to the same shape. We have the following rules for broadcasting:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Rule 1\"}],\": Dimension of size 1 can be broadcasted to any shape.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Rule 2\"}],\": Extra dimensions of 1 can be added with \",[\"$\",\"code\",null,{\"children\":\"view\"}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Rule 2\"}],\": Zip automatically adds dims of size 1 on the left.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Therefore, we can implement the \",[\"$\",\"code\",null,{\"children\":\"shape_broadcast\"}],\" method as follows:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"$9\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"For example, in the case of linear model, we have the data \",[\"$\",\"code\",null,{\"children\":\"X\"}],\" of shape \",[\"$\",\"code\",null,{\"children\":\"(B * F)\"}],\", the weight \",[\"$\",\"code\",null,{\"children\":\"W\"}],\" of shape \",[\"$\",\"code\",null,{\"children\":\"(F * H)\"}],\", and the bias \",[\"$\",\"code\",null,{\"children\":\"b\"}],\" of shape \",[\"$\",\"code\",null,{\"children\":\"(H)\"}],\". To compute the output \",[\"$\",\"code\",null,{\"children\":\"Y\"}],\" following \",[\"$\",\"code\",null,{\"children\":\"Y = X @ W + b\"}],\", we need to follow the following steps (suppose we don't have matrix multiplication implemented):\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"/projects/minitorch/broadcast.png\",\"alt\":\"Broadcasting\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"3. Parallel Computation on CPU and GPU\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"[To Be Updated]\"}]]],null],null]},[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"project\",\"children\",\"minitorch\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"project\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],\"params\":{}}],null],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__className_587f35\",\"children\":[\"$\",\"$Ld\",null,{\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/2a1054080b77ae12.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/b16e5c8f629bec3c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}]}]]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Yufan Zhang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Yufan Zhang's personal website\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n7:null\n"])</script></body></html>