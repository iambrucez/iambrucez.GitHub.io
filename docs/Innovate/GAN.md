---
tags:
    - GAN
    - ComputerVision
---

# A Glance into Generative Adversarial Network

<!-- ![GAN](../img/innovate/I-GAN.jpg) -->

> Author: [Junkai Man](https://keon.im), Xinyu Tian, Yufan Zhang & Zesen Zhuang
> 
> Instructor: [Prof. Elchanan Solomon](https://elchanansolomon.com/)
> 
> December 2021, Duke Kunshan University

![GAN](../img/innovate/I-GAN.jpg)

## 1 Goal

### 1.1 Introduction

Initially proposed by Ian J. Goodfellow and his colleagues in 2014, Generative Adversarial Network (GAN) is judged as one of the most interesting ideas in Machine Learning in recent years. It described an innovative unsupervised learning method that can generate highly indistinguishable statistics as the original dataset by making two networks contesting with each other.

Since its publication, the GAN has drawn a lot of attention from researchers and engineers who are continuously contributing to the optimization and applications of this new machine learning technique. New variations emerge in recent years. In this report, we primarily focus on the original GAN theory that was put forward in 2014 and replicate an optimized Deep Convolutional Generative Adversarial Network structure (DCGAN) to showcase one of its application of generating fabricated images of cats and CryptoPunks using collected dataset.

### 1.2 Generative and Discriminative Model

The GAN falls in the framework of Generative Model. It is in the contrast to the concept of “Discrim- inative Model” which focuses on choosing the decision boundary between data, while the “Generative Model” intends to describe the probability distribution of a dataset. This makes generative models harder to be trained. For example, discriminative model can directly draw one hyperplane in the data space to divide two clusters but the generative model has to generate the probability distribution throughout the whole data space.

Mathematically, discriminative model tries to learn the conditional probability distribution of p(Y |X) where Y denotes the labels and X denotes the predictor. But generative model tries to learn the joint probability of p(X, Y ) with the help of Bayes Theorem. For the unsupervised learning tasks where there’s no labels Y , it directly learns the probability distribution of p(X).

Before the GAN, there was already a family of generative models including ``Fully Visible Belief Network'' \cite{frey_hinton_dayan_1995}, ``Boltzmann Machines'' \cite{hinton2007boltzmann}, and ``Variational Autoencoders'' \cite{kingma_welling_2013}. What motivated the author to innovate the GAN was that these models usually involved the Markov Chain which has a higher computational cost and does not perform well in high dimensional spaces \cite{goodfellow2016nips}. The GAN shows its efficiency via the adversarial approach as described below.

### 1.3 Adversarial Approach

The adversarial approach is the highlight of the GAN and is taking inspiration from the Game Theory. The GAN acts like a ``Zero-sum Non-cooperative Game''. It describes a Minimax Game where two competitors are trying to maximize their own interest and minimizing the other's interest by all means. And through this process, GAN parallelly trains two neural networks to compete and finally the model will learn to generate results which are hard to distinguish whether real or fake. This is especially powerful when dealing with image and audio data, and it also shows a good result in practice.

## 2 Model

GANs are deep generative models with adversarial nets. Each of them contains a generator $G$ who generates new data, and a discriminator $D$ who estimates the data generated by $G$ with some given datasets to give feedback for $G$. It operates a minimax game where the generator and discriminator learn from each other and thus both sides are trained to be stronger. It finally produces new data that could be taken as real. This section generally uses Goodfellow's paper on GAN \cite{goodfellow_pouget-abadie_mirza_xu_warde-farley_ozair_courville_bengio_2014}, Google Developers \cite{google_developers_2019}, and Radford's paper on DCGAN \cite{radford_metz_chintala_2015}, as the references.

### 2.1 A General Picture

A GAN consists of two neural networks: a generator $G$, and a discriminator $D$.

- Generator $G$: Initially, the generator takes a random noise as its input and generate data; In the training process, the generator takes the feedback from the discriminator and upates itself to generate indistinguishable data.
- Discriminator $D$: The discriminator takes the real data and generated data as its input and do classification to determine the probability that the generated data is real, and give feedback to the generator.

For a generative task, in terms of the functionalities of the two components, a GAN model acts in 4 steps before the training process. Figure 1 shows the first two steps of a GAN model.



